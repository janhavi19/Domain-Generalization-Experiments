{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class ECG_FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECG_FeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 64)\n",
    "        return x\n",
    "\n",
    "class ECG_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECG_Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERM Alogorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Algorithm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A subclass of Algorithm implements a domain generalization algorithm.\n",
    "    Subclasses should implement the following:\n",
    "    - update()\n",
    "    - predict()\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, num_classes, num_domains, hparams):\n",
    "        super(Algorithm, self).__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "    def update(self, minibatches, unlabeled=None):\n",
    "        \"\"\"\n",
    "        Perform one update step, given a list of (x, y) tuples for all\n",
    "        environments.\n",
    "        Admits an optional list of unlabeled minibatches from the test domains,\n",
    "        when task is domain_adaptation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ERM(Algorithm):\n",
    "    \"\"\"\n",
    "    Empirical Risk Minimization (ERM)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, num_classes, num_domains, hparams):\n",
    "        super(ERM, self).__init__(input_shape, num_classes, num_domains, hparams)\n",
    "        self.featurizer = ECG_FeatureExtractor()\n",
    "        self.classifier = ECG_Classifier()\n",
    "        self.network = nn.Sequential(self.featurizer, self.classifier)\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.network.parameters(),\n",
    "            lr=self.hparams[\"lr\"],\n",
    "            weight_decay=self.hparams['weight_decay']\n",
    "        )\n",
    "\n",
    "    def update(self, minibatches, unlabeled=None):\n",
    "        \"\"\"\n",
    "        Perform one update step, given a list of (x, y) tuples for all\n",
    "        environments.\n",
    "        Admits an optional list of unlabeled minibatches from the test domains,\n",
    "        when task is domain_adaptation.\n",
    "        \"\"\"\n",
    "        self.network.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        all_x = torch.cat([x for x, y in minibatches])\n",
    "        all_y = torch.cat([y for x, y in minibatches])\n",
    "        \n",
    "        y_hat = self.network(all_x)\n",
    "        loss = F.cross_entropy(y_hat, all_y)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            x = x.unsqueeze(1)\n",
    "            y_hat = self.network(x)\n",
    "            return y_hat.argmax(dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\puranik\\Anaconda3\\envs\\pytorch_venv\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataloading import MultitaskDataset\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pickle\n",
    "import zarr\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_y(x):\n",
    "    mapping = {1: 0, 2: 1, 3: 2, 4: 3}\n",
    "    return mapping.get(x, x)\n",
    "\n",
    "def replace_d(x):\n",
    "    mapping = {2: 0, 3: 1, 4: 2, 5: 3, 6: 4, 7: 5, 8: 6, 9: 7, 10: 8, 11: 9, 13: 10, 14: 11, 15: 12, 16: 13, 17: 14}\n",
    "    return mapping.get(x, x)\n",
    "\n",
    "def loso(X,y,d):\n",
    "    left_out_subject = 14\n",
    "    idx = (d != left_out_subject)\n",
    "    X_train = X[idx]\n",
    "    y_train = y[idx]\n",
    "    d_train = d[idx]\n",
    "\n",
    "    # test data selecting just 14 subject\n",
    "    idxt = (d == left_out_subject)\n",
    "    X_test = X[idxt]\n",
    "    y_test = y[idxt]\n",
    "    d_test = d[idxt]\n",
    "\n",
    "\n",
    "    return X_train, y_train, d_train, X_test, y_test, d_test\n",
    "\n",
    "def data_loader():\n",
    "    zarr_array = zarr.open(\"./dataset/chest_ECG_w60_mw60_ts256_cl2_cs1_fp[1.0].zarr/\", mode=\"r\")\n",
    "    signal = zarr_array['raw_signal'][:]\n",
    "    target_all = zarr_array['target'][:]\n",
    "    subjects_all = zarr_array['subject'][:]\n",
    "\n",
    "    SUBJECTS_IDS = list(range(2, 18))\n",
    "    subjects = SUBJECTS_IDS[:]\n",
    "    classes = [1, 2, 3, 4]\n",
    "\n",
    "    subset_map = [\n",
    "        idx\n",
    "        for idx, i in enumerate(target_all)\n",
    "        if i in classes and subjects_all[idx] in subjects\n",
    "    ]\n",
    "\n",
    "    idx = subset_map\n",
    "    X = signal[idx]\n",
    "    y = target_all[idx]\n",
    "    d = subjects_all[idx]\n",
    "\n",
    "    y_updated = np.vectorize(replace_y)(y)\n",
    "    d_updated = np.vectorize(replace_d)(d)\n",
    "\n",
    "    X_train, y_train, d_train, X_test, y_test, d_test = loso(X,y_updated,d_updated)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    multitask_data = MultitaskDataset(X_train_scaled, y_train, d_train, X_test_scaled, y_test, d_test)\n",
    "    trainloader = multitask_data.train_loader(batch_size=32, shuffle=False)\n",
    "    testloader = multitask_data.test_loader(batch_size=32, shuffle=False)\n",
    "\n",
    "          \n",
    "    return  trainloader,testloader\n",
    "\n",
    "trainloader, testloader = data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {'lr': 0.001, 'weight_decay': 0.0001}\n",
    "alg = ERM(input_shape=(60, 1), num_classes=4, num_domains=14, hparams=hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_minibatches(dataset, batch_size):\n",
    "    minibatches = []\n",
    "    for env in dataset:\n",
    "        n_samples = len(env)\n",
    "        n_batches = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, n_samples)\n",
    "\n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "\n",
    "            for j in range(start_idx, end_idx):\n",
    "                X_batch.append(env[j][0])\n",
    "                y_batch.append(env[j][1])\n",
    "\n",
    "            minibatches.append((torch.stack(X_batch), torch.tensor(y_batch)))\n",
    "\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = make_minibatches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (x, y,d) in enumerate(dataloader):\n",
    "        x, y, d = data\n",
    "        minibatches = [(x, y)]\n",
    "        loss = alg.update(minibatches)\n",
    "        running_loss += loss\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {running_loss / (i+1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 39.295212765957444%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        x, y, d = data\n",
    "\n",
    "        \n",
    "     \n",
    "        outputs = alg.predict(x)\n",
    "        \n",
    "        outputs = outputs.unsqueeze(1)\n",
    "         \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "print(f\"Accuracy on test set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# get predictions for the test set\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for data in testloader:\n",
    "    x, y, d = data\n",
    "    \n",
    "    outputs = alg.predict(x)\n",
    "    outputs = outputs.unsqueeze(1)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    print(predicted)\n",
    "    y_true.extend(y.numpy())\n",
    "    y_pred.extend(predicted.numpy())\n",
    "\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# plot the confusion matrix\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       xticklabels=['class 0', 'class 1', 'class 2', 'class 3'],\n",
    "       yticklabels=['class 0', 'class 1', 'class 2', 'class 3'],\n",
    "       ylabel='True label',\n",
    "       xlabel='Predicted label')\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        text = ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                       ha=\"center\", va=\"center\", color=\"white\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
